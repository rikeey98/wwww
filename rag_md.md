# ë§ìŠµë‹ˆë‹¤! ì •í™•íˆ ì´í•´í•˜ì…¨ì–´ìš”

---

## 1. Ollamaë¡œ ë¡œì»¬ ì„¤ì¹˜ â†’ URL ìƒì„±

```bash
# 1. Ollama ì„¤ì¹˜ (ì´ë¯¸ í–ˆìœ¼ë©´ ìŠ¤í‚µ)
# Windows: https://ollama.com/download

# 2. qwen3-embedding ëª¨ë¸ ë‹¤ìš´ë¡œë“œ
ollama pull qwen3-embedding:8b

# 3. ìë™ìœ¼ë¡œ ì„œë²„ ì‹¤í–‰ë¨
# URL: http://localhost:11434/v1
```

**ì´ì œ ì´ URLì„ OpenAI APIì²˜ëŸ¼ ì‚¬ìš© ê°€ëŠ¥!**

---

## 2. LangChainì—ì„œ RAG êµ¬ì¶•

### RAGë€?
**ë¬¸ì„œ â†’ Vector DB ë³€í™˜ â†’ ìœ ì‚¬ë„ ê²€ìƒ‰ â†’ LLM ë‹µë³€**

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚        RAG ì „ì²´ íë¦„                â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                      â”‚
â”‚  [1] ë¬¸ì„œ ì¤€ë¹„                      â”‚
â”‚      â”œâ”€ MD íŒŒì¼ë“¤                   â”‚
â”‚      â””â”€ PDF, TXT ë“±                 â”‚
â”‚           â†“                          â”‚
â”‚  [2] Embedding ëª¨ë¸ (qwen3)         â”‚
â”‚      â”œâ”€ ë¬¸ì„œë¥¼ ë²¡í„°ë¡œ ë³€í™˜          â”‚
â”‚      â””â”€ http://localhost:11434/v1   â”‚
â”‚           â†“                          â”‚
â”‚  [3] Vector DB ì €ì¥                 â”‚
â”‚      â”œâ”€ FAISS (ë¡œì»¬ íŒŒì¼)           â”‚
â”‚      â”œâ”€ Chroma (SQLite)             â”‚
â”‚      â””â”€ Qdrant (Docker)             â”‚
â”‚           â†“                          â”‚
â”‚  [4] ì§ˆë¬¸ ì‹œ ê²€ìƒ‰                   â”‚
â”‚      â”œâ”€ ì§ˆë¬¸ì„ ë²¡í„°ë¡œ ë³€í™˜          â”‚
â”‚      â”œâ”€ ìœ ì‚¬í•œ ë¬¸ì„œ ì°¾ê¸°            â”‚
â”‚      â””â”€ ìƒìœ„ Kê°œ ë¬¸ì„œ ë°˜í™˜          â”‚
â”‚           â†“                          â”‚
â”‚  [5] LLM ë‹µë³€ ìƒì„±                  â”‚
â”‚      â””â”€ ì°¾ì€ ë¬¸ì„œ + ì§ˆë¬¸ â†’ ë‹µë³€    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## 3. ì „ì²´ ì½”ë“œ ì˜ˆì‹œ

```python
from langchain_openai import OpenAIEmbeddings, ChatOpenAI
from langchain_community.document_loaders import DirectoryLoader
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_community.vectorstores import FAISS
from langchain.chains import RetrievalQA

# =============================================
# 1. Embedding ëª¨ë¸ ì„¤ì • (ë¡œì»¬ Ollama)
# =============================================
embeddings = OpenAIEmbeddings(
    model="qwen3-embedding:8b",
    openai_api_base="http://localhost:11434/v1",
    openai_api_key="ollama"  # ë”ë¯¸ê°’
)

# =============================================
# 2. ë¬¸ì„œ ë¡œë“œ
# =============================================
loader = DirectoryLoader(
    "docs/",
    glob="**/*.md"
)
documents = loader.load()

# =============================================
# 3. ë¬¸ì„œ ë¶„í•  (ì²­í¬ ë‹¨ìœ„)
# =============================================
text_splitter = RecursiveCharacterTextSplitter(
    chunk_size=1000,
    chunk_overlap=200
)
chunks = text_splitter.split_documents(documents)

print(f"ë¬¸ì„œ {len(documents)}ê°œ â†’ ì²­í¬ {len(chunks)}ê°œë¡œ ë¶„í• ")

# =============================================
# 4. Vector DB ìƒì„± (ë¬¸ì„œë¥¼ ë²¡í„°ë¡œ ë³€í™˜í•˜ì—¬ ì €ì¥)
# =============================================
vectorstore = FAISS.from_documents(
    documents=chunks,
    embedding=embeddings
)

# ë””ìŠ¤í¬ì— ì €ì¥ (ì¬ì‚¬ìš© ê°€ëŠ¥)
vectorstore.save_local("faiss_index")

print("âœ… Vector DB ìƒì„± ì™„ë£Œ!")

# =============================================
# 5. LLM ì„¤ì • (ë‹µë³€ ìƒì„±ìš©)
# =============================================
llm = ChatOpenAI(
    model="gpt-4o-mini",
    openai_api_key="sk-xxx"
)

# =============================================
# 6. RAG ì²´ì¸ êµ¬ì„±
# =============================================
qa_chain = RetrievalQA.from_chain_type(
    llm=llm,
    chain_type="stuff",
    retriever=vectorstore.as_retriever(
        search_kwargs={"k": 3}  # ìƒìœ„ 3ê°œ ë¬¸ì„œ ê²€ìƒ‰
    )
)

# =============================================
# 7. ì§ˆë¬¸í•˜ê¸°
# =============================================
question = "MEM-001 íŒ¨í„´ì´ ë­ì•¼?"

result = qa_chain.invoke({"query": question})

print(result["result"])
```

---

## 4. ì‹¤í–‰ íë¦„ ìƒì„¸

### â‘  ë¬¸ì„œ ë¡œë“œ & ë¶„í• 
```python
# 100ê°œ MD íŒŒì¼ â†’ 500ê°œ ì²­í¬
documents = loader.load()  # [Doc1, Doc2, ...]
chunks = text_splitter.split_documents(documents)  # [Chunk1, Chunk2, ...]
```

### â‘¡ Vector DB ìƒì„± (í•µì‹¬!)
```python
# ë‚´ë¶€ ë™ì‘:
for chunk in chunks:
    # Ollama API í˜¸ì¶œ
    response = requests.post(
        "http://localhost:11434/v1/embeddings",
        json={"input": chunk.page_content, "model": "qwen3-embedding:8b"}
    )
    vector = response.json()["data"][0]["embedding"]
    # [0.123, -0.456, ..., 0.789]
    
    # FAISSì— ì €ì¥
    vectorstore.add(vector, metadata=chunk.metadata)
```

**ê²°ê³¼:** `faiss_index/` í´ë”ì— ì €ì¥ë¨

### â‘¢ ì§ˆë¬¸ ì‹œ ê²€ìƒ‰
```python
# 1. ì§ˆë¬¸ì„ ë²¡í„°ë¡œ ë³€í™˜
question_vector = embeddings.embed_query("MEM-001ì´ ë­ì•¼?")

# 2. ìœ ì‚¬ë„ ê³„ì‚° (ì½”ì‚¬ì¸ ìœ ì‚¬ë„)
similar_docs = vectorstore.similarity_search(
    "MEM-001ì´ ë­ì•¼?",
    k=3
)

# 3. ê²°ê³¼
# [
#   Document(page_content="MEM-001: Cache coherency violation...", metadata={"source": "patterns.md"}),
#   Document(page_content="MEM-001 ì‹¬ê°ë„ëŠ” 9ì…ë‹ˆë‹¤...", metadata={"source": "severity.md"}),
#   ...
# ]
```

### â‘£ LLM ë‹µë³€ ìƒì„±
```python
# ì°¾ì€ ë¬¸ì„œ + ì§ˆë¬¸ì„ í”„ë¡¬í”„íŠ¸ë¡œ ì¡°í•©
prompt = f"""
ë‹¤ìŒ ë¬¸ì„œë¥¼ ì°¸ê³ í•´ì„œ ì§ˆë¬¸ì— ë‹µí•˜ì„¸ìš”:

ë¬¸ì„œ1: {similar_docs[0].page_content}
ë¬¸ì„œ2: {similar_docs[1].page_content}
ë¬¸ì„œ3: {similar_docs[2].page_content}

ì§ˆë¬¸: MEM-001ì´ ë­ì•¼?
"""

# LLM í˜¸ì¶œ
answer = llm.invoke(prompt)
```

---

## 5. Vector DB ì¬ì‚¬ìš©

```python
# í•œ ë²ˆ ìƒì„±í•œ Vector DBëŠ” ì¬ì‚¬ìš© ê°€ëŠ¥
from langchain_community.vectorstores import FAISS

# ë¡œë“œë§Œ í•˜ë©´ ë¨ (ë‹¤ì‹œ ì„ë² ë”© ì•ˆí•´ë„ ë¨!)
vectorstore = FAISS.load_local(
    "faiss_index",
    embeddings=embeddings,
    allow_dangerous_deserialization=True
)

# ë°”ë¡œ ê²€ìƒ‰ ê°€ëŠ¥
results = vectorstore.similarity_search("MEM-001", k=3)
```

---

## 6. ë‹¹ì‹  í”„ë¡œì íŠ¸ ì ìš©

### í´ë” êµ¬ì¡°
```
soc_automation/
â”œâ”€â”€ config/
â”‚   â””â”€â”€ prompts/
â”‚       â””â”€â”€ error_analyzer/
â”‚           â””â”€â”€ sub_agents/
â”‚               â”œâ”€â”€ pattern_matcher.md       # 100+ íŒ¨í„´
â”‚               â”œâ”€â”€ severity_assessor.md
â”‚               â””â”€â”€ root_cause_analyzer.md
â””â”€â”€ rag/
    â”œâ”€â”€ build_vectordb.py     # Vector DB ìƒì„± ìŠ¤í¬ë¦½íŠ¸
    â”œâ”€â”€ faiss_index/          # ì €ì¥ëœ Vector DB
    â””â”€â”€ search.py             # ê²€ìƒ‰ í•¨ìˆ˜
```

### Vector DB ìƒì„± ìŠ¤í¬ë¦½íŠ¸
```python
# soc_automation/rag/build_vectordb.py

from langchain_openai import OpenAIEmbeddings
from langchain_community.document_loaders import DirectoryLoader
from langchain_text_splitters import MarkdownTextSplitter
from langchain_community.vectorstores import FAISS

# 1. Ollama Embedding
embeddings = OpenAIEmbeddings(
    model="qwen3-embedding:8b",
    openai_api_base="http://localhost:11434/v1",
    openai_api_key="ollama"
)

# 2. MD íŒŒì¼ ë¡œë“œ
loader = DirectoryLoader(
    "config/prompts/error_analyzer/sub_agents/",
    glob="**/*.md",
    loader_cls=lambda path: MarkdownTextSplitter().create_documents([open(path).read()])
)
documents = loader.load()

# 3. Vector DB ìƒì„±
vectorstore = FAISS.from_documents(documents, embeddings)

# 4. ì €ì¥
vectorstore.save_local("rag/faiss_index")

print(f"âœ… {len(documents)}ê°œ ë¬¸ì„œ â†’ Vector DB ìƒì„± ì™„ë£Œ")
```

### ê²€ìƒ‰ í•¨ìˆ˜
```python
# soc_automation/rag/search.py

from langchain_openai import OpenAIEmbeddings
from langchain_community.vectorstores import FAISS

def search_patterns(error_message: str, k: int = 5) -> list:
    """ì—ëŸ¬ ë©”ì‹œì§€ë¡œ ìœ ì‚¬í•œ íŒ¨í„´ ê²€ìƒ‰"""
    
    # Embedding ì„¤ì •
    embeddings = OpenAIEmbeddings(
        model="qwen3-embedding:8b",
        openai_api_base="http://localhost:11434/v1",
        openai_api_key="ollama"
    )
    
    # Vector DB ë¡œë“œ
    vectorstore = FAISS.load_local(
        "rag/faiss_index",
        embeddings=embeddings,
        allow_dangerous_deserialization=True
    )
    
    # ê²€ìƒ‰
    results = vectorstore.similarity_search(
        error_message,
        k=k
    )
    
    return results

# ì‚¬ìš© ì˜ˆì‹œ
if __name__ == "__main__":
    results = search_patterns("Cache coherency violation detected")
    
    for doc in results:
        print(doc.page_content[:200])
        print(doc.metadata)
        print("---")
```

### Agentì—ì„œ ì‚¬ìš©
```python
# soc_automation/agents/error_analyzer/sub_agents/pattern_matcher.py

from soc_automation.rag.search import search_patterns

async def pattern_matcher_node(state):
    error_message = state['error_message']
    
    # RAGë¡œ ìœ ì‚¬ íŒ¨í„´ ê²€ìƒ‰
    similar_patterns = search_patterns(error_message, k=5)
    
    # LLMì—ê²Œ ê²€ìƒ‰ ê²°ê³¼ì™€ í•¨ê»˜ ì „ë‹¬
    prompt = f"""
    ì—ëŸ¬ ë©”ì‹œì§€: {error_message}
    
    ìœ ì‚¬í•œ íŒ¨í„´ë“¤:
    {chr(10).join([doc.page_content for doc in similar_patterns])}
    
    ìœ„ íŒ¨í„´ ì¤‘ ê°€ì¥ ì¼ì¹˜í•˜ëŠ” ê²ƒì„ ì„ íƒí•˜ì„¸ìš”.
    """
    
    # LLM í˜¸ì¶œ...
```

---

## 7. ì •ë¦¬

| ë‹¨ê³„ | ì„¤ëª… | í•œ ë²ˆë§Œ? |
|------|------|----------|
| **Ollama ì„¤ì¹˜** | `ollama pull qwen3-embedding:8b` | âœ… í•œ ë²ˆë§Œ |
| **Vector DB ìƒì„±** | ë¬¸ì„œ â†’ ë²¡í„° ë³€í™˜ â†’ ì €ì¥ | âœ… í•œ ë²ˆë§Œ |
| **ê²€ìƒ‰** | ì§ˆë¬¸ â†’ ë²¡í„° ë³€í™˜ â†’ ìœ ì‚¬ë„ ê²€ìƒ‰ | ë§¤ë²ˆ |
| **LLM ë‹µë³€** | ê²€ìƒ‰ ê²°ê³¼ + ì§ˆë¬¸ â†’ ë‹µë³€ | ë§¤ë²ˆ |

**RAG = Vector DB ë§Œë“¤ê¸° + ìœ ì‚¬ë„ ê²€ìƒ‰ + LLM ë‹µë³€**

ë‹¹ì‹ ì´ ì´í•´í•œ ê²ƒì´ 100% ë§ìŠµë‹ˆë‹¤! ğŸ¯
